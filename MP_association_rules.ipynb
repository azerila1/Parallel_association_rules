{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def association_rules_MP(df_subset,main_set,\n",
    "                         metric=\"confidence\",\n",
    "                         min_threshold=0.8,\n",
    "                         support_only=False):\n",
    "          \n",
    "    \"\"\"Generates a DataFrame of association rules including the\n",
    "    metrics 'score', 'confidence', and 'lift'\n",
    "    Parameters\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "      pandas DataFrame of frequent itemsets\n",
    "      with columns ['support', 'itemsets']\n",
    "    metric : string (default: 'confidence')\n",
    "      Metric to evaluate if a rule is of interest.\n",
    "      **Automatically set to 'support' if `support_only=True`.**\n",
    "      Otherwise, supported metrics are 'support', 'confidence', 'lift',\n",
    "      'leverage', and 'conviction'\n",
    "      These metrics are computed as follows:\n",
    "      - support(A->C) = support(A+C) [aka 'support'], range: [0, 1]\\n\n",
    "      - confidence(A->C) = support(A+C) / support(A), range: [0, 1]\\n\n",
    "      - lift(A->C) = confidence(A->C) / support(C), range: [0, inf]\\n\n",
    "      - leverage(A->C) = support(A->C) - support(A)*support(C),\n",
    "        range: [-1, 1]\\n\n",
    "      - conviction = [1 - support(C)] / [1 - confidence(A->C)],\n",
    "        range: [0, inf]\\n\n",
    "    min_threshold : float (default: 0.8)\n",
    "      Minimal threshold for the evaluation metric,\n",
    "      via the `metric` parameter,\n",
    "      to decide whether a candidate rule is of interest.\n",
    "    support_only : bool (default: False)\n",
    "      Only computes the rule support and fills the other\n",
    "      metric columns with NaNs. This is useful if:\n",
    "      a) the input DataFrame is incomplete, e.g., does\n",
    "      not contain support values for all rule antecedents\n",
    "      and consequents\n",
    "      b) you simply want to speed up the computation because\n",
    "      you don't need the other metrics.\n",
    "    Returns\n",
    "    ----------\n",
    "    pandas DataFrame with columns \"antecedents\" and \"consequents\"\n",
    "      that store itemsets, plus the scoring metric columns:\n",
    "      \"antecedent support\", \"consequent support\",\n",
    "      \"support\", \"confidence\", \"lift\",\n",
    "      \"leverage\", \"conviction\"\n",
    "      of all rules for which\n",
    "      metric(rule) >= min_threshold.\n",
    "      Each entry in the \"antecedents\" and \"consequents\" columns are\n",
    "      of type `frozenset`, which is a Python built-in type that\n",
    "      behaves similarly to sets except that it is immutable\n",
    "      (For more info, see\n",
    "      https://docs.python.org/3.6/library/stdtypes.html#frozenset).\n",
    "    Examples\n",
    "    -----------\n",
    "    For usage examples, please see\n",
    "    http://rasbt.github.io/mlxtend/user_guide/frequent_patterns/association_rules/\n",
    "    \"\"\"\n",
    "\n",
    "    # check for mandatory columns\n",
    "    if not all(col in main_set.columns for col in [\"support\", \"itemsets\"]):\n",
    "        raise ValueError(\"Dataframe needs to contain the\\\n",
    "                         columns 'support' and 'itemsets'\")\n",
    "\n",
    "    def conviction_helper(sAC, sA, sC):\n",
    "        confidence = sAC/sA\n",
    "        conviction = np.empty(confidence.shape, dtype=float)\n",
    "        if not len(conviction.shape):\n",
    "            conviction = conviction[np.newaxis]\n",
    "            confidence = confidence[np.newaxis]\n",
    "            sAC = sAC[np.newaxis]\n",
    "            sA = sA[np.newaxis]\n",
    "            sC = sC[np.newaxis]\n",
    "        conviction[:] = np.inf\n",
    "        conviction[confidence < 1.] = ((1. - sC[confidence < 1.]) /\n",
    "                                       (1. - confidence[confidence < 1.]))\n",
    "\n",
    "        return conviction\n",
    "\n",
    "    # metrics for association rules\n",
    "    metric_dict = {\n",
    "        \"antecedent support\": lambda _, sA, __: sA,\n",
    "        \"consequent support\": lambda _, __, sC: sC,\n",
    "        \"support\": lambda sAC, _, __: sAC,\n",
    "        \"confidence\": lambda sAC, sA, _: sAC/sA,\n",
    "        \"lift\": lambda sAC, sA, sC: metric_dict[\"confidence\"](sAC, sA, sC)/sC,\n",
    "        \"leverage\": lambda sAC, sA, sC: metric_dict[\"support\"](\n",
    "             sAC, sA, sC) - sA*sC,\n",
    "        \"conviction\": lambda sAC, sA, sC: conviction_helper(sAC, sA, sC)\n",
    "        }\n",
    "\n",
    "    columns_ordered = [\"antecedent support\", \"consequent support\",\n",
    "                       \"support\",\n",
    "                       \"confidence\", \"lift\",\n",
    "                       \"leverage\", \"conviction\"]\n",
    "\n",
    "    # check for metric compliance\n",
    "    if support_only:\n",
    "        metric = 'support'\n",
    "    else:\n",
    "        if metric not in metric_dict.keys():\n",
    "            raise ValueError(\"Metric must be 'confidence' or 'lift', got '{}'\"\n",
    "                             .format(metric))\n",
    "\n",
    "    # get dict of {frequent itemset} -> support\n",
    "    keys = main_set['itemsets'].values\n",
    "    values = main_set['support'].values\n",
    "    frozenset_vect = np.vectorize(lambda x: frozenset(x))\n",
    "    frequent_items_dict = dict(zip(frozenset_vect(keys), values))\n",
    "    \n",
    "    ############################ added on 23.10.2018 : Alireza Ranjbar \n",
    "    ############################ for Parallel computring\n",
    "    keys_SUBSETTT = df_subset['itemsets'].values\n",
    "    values_SUBSETTT = df_subset['support'].values\n",
    "    frozenset_vect_SUBSETTT = np.vectorize(lambda x: frozenset(x))\n",
    "    frequent_items_dict_SUBSETTT = dict(zip(frozenset_vect(keys_SUBSETTT), values_SUBSETTT))\n",
    "    ############################\n",
    "    ############################\n",
    "    \n",
    "    # prepare buckets to collect frequent rules\n",
    "    rule_antecedents = []\n",
    "    rule_consequents = []\n",
    "    rule_supports = []\n",
    "\n",
    "    # iterate over all frequent itemsets\n",
    "    for k in frequent_items_dict_SUBSETTT.keys():\n",
    "        sAC = frequent_items_dict[k]\n",
    "        # to find all possible combinations\n",
    "        for idx in range(len(k)-1, 0, -1):\n",
    "            # of antecedent and consequent\n",
    "            for c in combinations(k, r=idx):\n",
    "                antecedent = frozenset(c)\n",
    "                consequent = k.difference(antecedent)\n",
    "\n",
    "                if support_only:\n",
    "                    # support doesn't need these,\n",
    "                    # hence, placeholders should suffice\n",
    "                    sA = None\n",
    "                    sC = None\n",
    "\n",
    "                else:\n",
    "                    try:\n",
    "                        sA = frequent_items_dict[antecedent]\n",
    "                        sC = frequent_items_dict[consequent]\n",
    "                    except KeyError as e:\n",
    "                        s = (str(e) + 'You are likely getting this error'\n",
    "                                      ' because the DataFrame is missing '\n",
    "                                      ' antecedent and/or consequent '\n",
    "                                      ' information.'\n",
    "                                      ' You can try using the '\n",
    "                                      ' `support_only=True` option')\n",
    "                        raise KeyError(s)\n",
    "                    # check for the threshold\n",
    "\n",
    "                score = metric_dict[metric](sAC, sA, sC)\n",
    "                if score >= min_threshold:\n",
    "                    rule_antecedents.append(antecedent)\n",
    "                    rule_consequents.append(consequent)\n",
    "                    rule_supports.append([sAC, sA, sC])\n",
    "\n",
    "    # check if frequent rule was generated\n",
    "    if not rule_supports:\n",
    "        \n",
    "        output=pd.DataFrame( columns=[\"antecedents\", \"consequents\"] + columns_ordered)\n",
    "        #if len(output)==0:\n",
    "         #   output=pd.DataFrame([1 for i in range(9)],output.columns).T\n",
    "        return output\n",
    "\n",
    "    else:\n",
    "        # generate metrics\n",
    "        rule_supports = np.array(rule_supports).T.astype(float)\n",
    "        df_res = pd.DataFrame(\n",
    "            data=list(zip(rule_antecedents, rule_consequents)),\n",
    "            columns=[\"antecedents\", \"consequents\"])\n",
    "\n",
    "        if support_only:\n",
    "            sAC = rule_supports[0]\n",
    "            for m in columns_ordered:\n",
    "                df_res[m] = np.nan\n",
    "            df_res['support'] = sAC\n",
    "\n",
    "        else:\n",
    "            sAC = rule_supports[0]\n",
    "            sA = rule_supports[1]\n",
    "            sC = rule_supports[2]\n",
    "            for m in columns_ordered:\n",
    "                df_res[m] = metric_dict[m](sAC, sA, sC)\n",
    "         \n",
    "    df_res['antecedents'] = df_res['antecedents'].apply(lambda x: list(x))\n",
    "    df_res['consequents'] = df_res['consequents'].apply(lambda x: list(x))\n",
    "    \n",
    "    #if len(df_res)==0:\n",
    "     #   df_res=pd.DataFrame([1 for i in range(9)],df_res.columns).T\n",
    "\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "#association_rules_MP(df_subset=result[:2],main_set=result,\n",
    "#                         metric=\"confidence\",\n",
    "#                         min_threshold=0.8,\n",
    "#                         support_only=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
